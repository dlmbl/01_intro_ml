{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWC5tKyP3x1e"
   },
   "source": [
    "Some code cells will be marked with \n",
    "```\n",
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "```\n",
    "\n",
    "This indicates that you are being asked to write a piece of code to complete the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: The Linear Classifier\n",
    "\n",
    "To illustrate the workflow for training a deep learning model in a supervised manner, this notebook will walk you through the simple case of training a linear classifier to recognize images various stages of the cell cycle. While deep learning might seem intimidating, don't worry. Its conceptual underpinnings are rooted in linear algebra and calculus - if you can perform matrix multiplication and take derivatives you can understand what is happening in a deep learning workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DwKG_Gi3x1f"
   },
   "outputs": [],
   "source": [
    "import imageio as iio\n",
    "import skimage\n",
    "import sklearn.model_selection\n",
    "import sklearn.utils\n",
    "import sklearn.metrics\n",
    "import skimage.color\n",
    "import skimage.transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZ2jTjsd3x1g"
   },
   "source": [
    "## The supervised machine learning workflow\n",
    "Recall from class the conceptual workflow for a supervised machine learning project. \n",
    "- First, we create a <em>training dataset</em>, a paired collection of raw data and labels where the labels contain information about the \"insight\" we wish to extract from the raw data. \n",
    "- Once we have training data, we can then use it to train a <em>model</em>. The model is a mathematical black box - it takes in data and transforms it into an output. The model has some parameters that we can adjust to change how it performs this mapping. \n",
    "- Adjusting these parameters to produce outputs that we want is called training the model. To do this we need two things. First, we need a notion of what we want the output to look like. This notion is captured by a <em>loss function</em>, which compares model outputs and labels and produces a score telling us if the model did a \"good\" job or not on our given task. By convention, low values of the loss function's output (e.g. the loss) correspond to good performance and high values to bad performance. We also need an <em>optimization algorithm</em>, which is a set of rules for how to adjust the model parameters to reduce the loss\n",
    "- Using the training data, loss function, and optimization algorithm, we can then train the model \n",
    "- Once the model is trained, we need to evaluate its performance to see how well it performs and what kinds of mistakes it makes. We can also perform this kind of monitoring during training (this is actually a standard practice).\n",
    "\n",
    "Because this workflow defines the lifecycle of most machine learning projects, this notebook is structured to go over each of these steps while constructing a linear classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbIDLvJ23x1g"
   },
   "source": [
    "## Create training data\n",
    "The starting point of every machine learning project is data. Today we are going to look at a collection of images of Jurkat cells published in the Broad Bioimage Collection ([BBBC048](https://bbbc.broadinstitute.org/BBBC048)). The cells were fixed and stained with PI (propidium iodide) to quantify DNA content and a MPM2 (mitotic protein monoclonal #2) antibody to identify mitotic cells.\n",
    "\n",
    "During the initial setup of this exercise, we downloaded the data and unzipped the relevant files using the script `data-download.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/CellCycle'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command above should generate the following output. If you see something different, please check that the `data-download.sh` script ran correctly.\n",
    "```\n",
    "['img.lst~',\n",
    " 'Anaphase',\n",
    " 'Prophase',\n",
    " 'img.lst',\n",
    " 'S',\n",
    " '66.lst~',\n",
    " 'G1',\n",
    " 'Metaphase',\n",
    " 'G2',\n",
    " 'Telophase']\n",
    " ```\n",
    " \n",
    "The metadata for each file is stored in `img.lst` so we will first load this information to inform how we load the rest of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe with sample info\n",
    "df = pd.read_csv(os.path.join(data_dir, 'img.lst'), sep='\\t', header=None)\n",
    "df = df.rename(columns={1: 'class', 2: 'filepath'})\n",
    "df['channel'] = df['filepath'].str.split('/',expand=True)[2].str.split('_', expand=True)[1].str.slice(2,3)\n",
    "df['id'] = df['filepath'].str.split('/',expand=True)[2].str.split('_', expand=True)[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each `id` there are three images. One for each of the channels: phase, PI and MPM2. We will load each image and stack it into an array of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data stack\n",
    "ims = []\n",
    "ys = []\n",
    "for i, g in df.groupby('id'):\n",
    "    im = []\n",
    "    for _, r in g.iterrows():\n",
    "        im.append(iio.imread(os.path.join(data_dir, r['filepath'])))\n",
    "    ims.append(np.stack(im, axis=-1))\n",
    "    ys.append(r['class'])\n",
    "    \n",
    "X_data = np.stack(ims)\n",
    "y_data = np.stack(ys)\n",
    "print('X shape:', X_data.shape)\n",
    "print('y shape:', y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WG4mmJ173x1h"
   },
   "source": [
    "In the previous cell, you probably observed that there are 4 dimensions rather than the 3 you might have been expecting. This is because while each image is (66, 66, 3), the full dataset has many images. The different images are stacked along the first dimension. The full size of the training images is (# images, 66, 66, 3) - the first dimension is often called the batch dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "# Use matplotlib (plt.imshow) to visualize several images randomly drawn from the dataset\n",
    "# There are 6 classes in the dataset. Make sure to look at an example or each class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the balance of classes in this dataset. There are at least two ways you could do this. One would be to use matplotlib to create a histogram. The other would be to count the number of items in each class using numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "# Add your code to check class balances here\n",
    "# You should end up with a count of number of items in each of the 7 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is highly inbalanced so we will want to correct the class balance before training. We will resample the data after splitting it below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we are going to restrict the dataset to two classes `[3, 5]` and a single channel `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_classes(X_data, y_data, classes):\n",
    "    X, y = [], []\n",
    "    for c in classes:\n",
    "        # Identify the indicies of the relevant class\n",
    "        idx = y_data == c\n",
    "        # Select the X and y data accordingly\n",
    "        X.append(X_data[idx, ..., 0:1])\n",
    "        y.append(y_data[idx])\n",
    "\n",
    "    # Restack the arrays\n",
    "    X = np.concatenate(X)\n",
    "    y = np.concatenate(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = extract_classes(X_data, y_data, [3, 5])\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also reassign classes 3 and 5 to 0 and 1 for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y == 3] = 0\n",
    "y[y == 5] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D_YhVir3x1i"
   },
   "source": [
    "For this exercise, we will want to flatten the training data into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J3MJPnh-3x1j",
    "outputId": "c280504b-94c4-4888-dd02-875c789dee9f"
   },
   "outputs": [],
   "source": [
    "# Flatten the images 1d vectors\n",
    "X = np.reshape(X, (-1, 66 * 66, 1))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2yrGjOL3x1j"
   },
   "source": [
    "### Split the training dataset into training, validation, and testing datasets\n",
    "How do we know how well our model is doing? A common practice to evaluate models is to evaluate them on splits of the original training dataset. Splitting the data is important, because we want to see how models perform on data that was not used to train them.\n",
    "- The <em>training</em> dataset used to train the model\n",
    "- A held out <em>testing</em> dataset used to evaluate the final trained version of the model\n",
    "While there is no hard and fast rule, 80%, 20% splits are a reasonable starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLQUiSoj3x1j"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and testing splits\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.8, shuffle=True)\n",
    "unbalanced = {\n",
    "    'train': {'X': X_train, 'y': y_train},\n",
    "    'test': {'X': X_test, 'y': y_test}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is split into train and test splits we will upsample the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(X, y):\n",
    "    class_ids = np.unique(y)\n",
    "    \n",
    "    # Compute counts to determine minority class\n",
    "    if np.count_nonzero(y == class_ids[0]) >= np.count_nonzero(y == class_ids[1]):\n",
    "        max_id = 0\n",
    "        min_id = 1\n",
    "    else:\n",
    "        max_id = 1\n",
    "        min_id = 0\n",
    "        \n",
    "    # Split the classes into separate arrays\n",
    "    x_max = X[y == max_id]\n",
    "    y_max = y[y == max_id]\n",
    "    x_min = X[y == min_id]\n",
    "    y_min = y[y == min_id]\n",
    "    \n",
    "    # Upsample min class to match number of samples in max\n",
    "    x_min, y_min = sklearn.utils.resample(x_min, y_min, n_samples=x_max.shape[0])\n",
    "    \n",
    "    # Concatenate the arrays from different classes back together \n",
    "    X = np.concatenate([x_max, x_min])\n",
    "    y = np.concatenate([y_max, y_min])\n",
    "    \n",
    "    # Shuffle arrays to randomize sample order\n",
    "    X, y = sklearn.utils.shuffle(X, y)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced = {}\n",
    "for split, data in unbalanced.items():\n",
    "    xx, yy = balance_classes(data['X'], data['y'])\n",
    "\n",
    "    # Save into the new dictionary\n",
    "    balanced[split] = {'X': xx, 'y': yy}\n",
    "    print(split, xx.shape, yy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fhlPKpe3x1j"
   },
   "source": [
    "## The linear classifier\n",
    "The linear classifier produces class scores that are a linear function of the pixel values. Mathematically, this can be written as $\\vec{y} = W \\vec{x}$, where $\\vec{y}$ is the vector of class scores, $W$ is a matrix of weights and $\\vec{x}$ is the image vector. The shape of the weights matrix is determined by the number of classes and the length of the image vector. In this case $W$ is 2 by 4356. Our learning task is to find a set of weights that maximize our performance on our classification task. We will solve this task by doing the following steps\n",
    "- Randomly initializing a set of weights\n",
    "- Defining a loss function that measures our performance on the classification task\n",
    "- Use stochastic gradient descent to find \"optimal\" weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0LQ99KR3x1j"
   },
   "source": [
    "### Create the matrix of weights\n",
    "Properly initializing weights is essential for getting deep learning methods to work correctly. The two most common initialization methods you'll see in this class are [glorot uniform (also known as Xavier) initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi]) and [he initialization](http://openaccess.thecvf.com/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html) - both papers are worth reading. For this exercise, we will randomly initialize weights by using glorot uniform initialization. In this initialization method, we sample our weights according to the formula \n",
    "\\begin{equation}\n",
    "W_{ij} \\sim U\\left[ -\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}} \\right],\n",
    "\\end{equation}\n",
    "where $n$ is the number of columns in the weight matrix (4,356 in our case).\n",
    "\n",
    "Lets create the linear classifier using object oriented programming, which will help with organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qL46rSt_gRJz",
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "class LinearClassifier(object):\n",
    "    def __init__(self, image_size=4356):\n",
    "        self.image_size=image_size\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        \n",
    "        ##########################\n",
    "        ######## To Do ###########\n",
    "        ##########################\n",
    "        \n",
    "        # Randomly initialize the weights matrix acccording to the glorot uniform initialization\n",
    "        # self.W = # Add weights matrix here\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yeA5geI3x1k"
   },
   "source": [
    "### Apply the softmax transform to complete the model outputs\n",
    "Our LinearClassifier class needs a method to perform predictions - which in our case is performing matrix multiplication and then applying the softmax transform. Recall from class that the softmax transform is given by\n",
    "\\begin{equation}\n",
    "softmax(y_i) = \\frac{e^{y_i}}{\\sum_j e^{y_j}}\n",
    "\\end{equation}\n",
    "and provides a convenient way to convert our class scores into probabilities.\n",
    "\n",
    "Note that our predict function is going to start by removing the channel dimension from the X data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YadGdARMgRJ0",
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## To Do ###########\n",
    "##########################\n",
    "\n",
    "# Complete the predict function below to predict a label y from an input X\n",
    "# Pay careful attention to the shape of your data at each step\n",
    "\n",
    "def predict(self, X, epsilon=1e-5):\n",
    "    X = X[..., 0]\n",
    "    pass\n",
    "    #y = # matrix multiplication\n",
    "\n",
    "    #y = # Apply softmax\n",
    "    return y\n",
    "\n",
    "# Assign methods to class\n",
    "setattr(LinearClassifier, 'predict', predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9l7Zum93x1l"
   },
   "source": [
    "Now lets see what happens when we try to predict the class of images in our training dataset using randomly initialized weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4tgJymU33x1l",
    "outputId": "736e325f-56eb-4bf7-9caf-bc6591c2a448"
   },
   "outputs": [],
   "source": [
    "lc = LinearClassifier()\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(20,20))\n",
    "for i, j in enumerate(np.random.randint(balanced['test']['X'].shape[0], size=(8,))):\n",
    "    \n",
    "    # Get an example image\n",
    "    X_sample = balanced['test']['X'][j,...]\n",
    "    \n",
    "    # Reshape flattened vector to image\n",
    "    X_reshape = np.reshape(X_sample, (66, 66))\n",
    "    \n",
    "    # Predict the label\n",
    "    y_pred = lc.predict(X_sample)\n",
    "    \n",
    "    # Display results\n",
    "    axes.flatten()[i].imshow(X_reshape, cmap='gray')\n",
    "    axes.flatten()[i].set_title('Label ' + str(balanced['test']['y'][j]) +', Prediction ' + str(np.argmax(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to inspecting the results of individual predictions, we can also look at summary statistics that capture model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_performance(y_true, y_pred):\n",
    "    \"\"\"Calculates recall, precision, f1 and a confusion matrix for sample predictions\"\"\"\n",
    "    \n",
    "    metrics = {\n",
    "        'recall': sklearn.metrics.recall_score(y_true, y_pred),\n",
    "        'precision': sklearn.metrics.precision_score(y_true, y_pred),\n",
    "        'f1': sklearn.metrics.f1_score(y_true, y_pred),\n",
    "        'cm': sklearn.metrics.confusion_matrix(y_true, y_pred, normalize='true'),\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Task 1.5\n",
    "For each of the 4 metrics above, describe in your own words what this metric tells you about model performance.\n",
    "\n",
    "########################  \n",
    "######## To Do ###########  \n",
    "########################  \n",
    "\n",
    "- Recall\n",
    "- Precision\n",
    "- F1\n",
    "- Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and metrics for training data\n",
    "y_pred = lc.predict(balanced['train']['X'])\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_true = balanced['train']['y']\n",
    "\n",
    "metrics = benchmark_performance(y_true, y_pred)\n",
    "\n",
    "print('Training Recall: {}'.format(metrics['recall']))\n",
    "print('Training Precision: {}'.format(metrics['precision']))\n",
    "print('Training F1 Score: {}'.format(metrics['f1']),'\\n')\n",
    "\n",
    "# Visualize a confusion matrix\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(metrics['cm'])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iKRfu3J3x1l",
    "tags": []
   },
   "source": [
    "#### Task 1.6\n",
    "What do you notice about the initial results of the model? \n",
    "\n",
    "########################  \n",
    "######## To Do ###########  \n",
    "########################  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jv4Rc_xS3x1l"
   },
   "source": [
    "## Stochastic gradient descent\n",
    "To train this model, we will use stochastic gradient descent. In its simplest version, this algorithm consists of the following steps:\n",
    "- Select several images from the training dataset at random\n",
    "- Compute the gradient of the loss function with respect to the weights, given the selected images\n",
    "- Update the weights using the update rule $\\Delta W_{ij} \\rightarrow \\Delta W_{ij} - lr\\frac{\\partial loss}{\\partial W_{ij}}$\n",
    "\n",
    "Recall that the origin of this update rule is from multivariable calculus - the gradient tells us the direction in which the loss function increases the most. So to minimize the loss function we move in the opposite direction of the gradient.\n",
    "\n",
    "Also recall from the course notes that for this problem we can compute the gradient analytically. The gradient is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial loss}{\\partial W_{ij}} = \\left(p_i - 1(i \\mbox{ is correct}) \\right)x_j,\n",
    "\\end{equation}\n",
    "where $1$ is an indicator function that is 1 if the statement inside the parentheses is true and 0 if it is false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXHGyfz33x1l"
   },
   "outputs": [],
   "source": [
    "def grad(self, X, y):\n",
    "    # Get class probabilities\n",
    "    p = self.predict(X)\n",
    "    \n",
    "    # Compute class 0 gradients\n",
    "    temp_0 = np.expand_dims(p[...,0] - (1-y), axis=-1)\n",
    "    grad_0 = temp_0 * X[...,0]\n",
    "\n",
    "    # Compute class 1 gradients\n",
    "    temp_1 = np.expand_dims(p[...,1] - y, axis=-1)\n",
    "    grad_1 =  temp_1 * X[...,0]\n",
    "    \n",
    "    gradient = np.stack([grad_0, grad_1], axis=1)\n",
    "    \n",
    "    return gradient\n",
    "    \n",
    "def loss(self, X, y_true):\n",
    "    y_pred = self.predict(X)\n",
    "    \n",
    "    # Convert y_true to one hot\n",
    "    y_true = np.stack([y_true, 1-y_true], axis=-1)\n",
    "    loss = np.mean(-y_true * np.log(y_pred))\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "def fit(self, X_train, y_train, n_epochs, batch_size=1, learning_rate=1e-5):\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = np.int(np.floor(X_train.shape[0] / batch_size))\n",
    "        \n",
    "        # Generate random index\n",
    "        index = np.arange(X_train.shape[0])\n",
    "        np.random.shuffle(index)\n",
    "        \n",
    "        # Iterate over batches\n",
    "        loss_list = []\n",
    "        for batch in range(n_batches):\n",
    "            beg = batch*batch_size\n",
    "            end = (batch+1)*batch_size if (batch+1)*batch_size < X_train.shape[0] else -1\n",
    "            X_batch = X_train[beg:end]\n",
    "            y_batch = y_train[beg:end]\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = self.loss(X_batch, y_batch)\n",
    "            loss_list.append(loss)\n",
    "            \n",
    "            # Compute the gradient\n",
    "            gradient = self.grad(X_batch, y_batch)\n",
    "            \n",
    "            # Compute the mean gradient over all the example images\n",
    "            gradient = np.mean(gradient, axis=0, keepdims=False)\n",
    "\n",
    "            # Update the weights\n",
    "            self.W -= learning_rate * gradient\n",
    "            \n",
    "        return loss_list\n",
    "\n",
    "# Assign methods to class\n",
    "setattr(LinearClassifier, 'grad', grad)\n",
    "setattr(LinearClassifier, 'loss', loss)\n",
    "setattr(LinearClassifier, 'fit', fit)\n",
    "\n",
    "lc = LinearClassifier()\n",
    "loss = lc.fit(balanced['train']['X'], balanced['train']['y'], n_epochs=32, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hh-GWNb-3x1m"
   },
   "source": [
    "## Evaluate the model\n",
    "Benchmarking performance is a critical part of the model development process. For this problem, we will use 3 different benchmarks\n",
    "- Recall: the fraction of positive examples detected by a model. Mathematically, for a two-class classification problem, recall is calculated as (True positives)/(True positives + False negatives). \n",
    "- Precision: the percentage of positive predictions from a model that are true. Mathematically, for a two-class prediction problem, precision is calculated as (True positives)/(True positives + False positives).\n",
    "- F1 score: The harmonic mean between the recall and precision\n",
    "\n",
    "We will evaluate these metrics on both the training dataset (the examples used during training) and our testing dataset (the examples that we held out). We can also use a confusion matrix to visualize the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "T_wsv4zW3x1m",
    "outputId": "1d5928f9-1acb-43ec-8c58-ad756e538171"
   },
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(20,20))\n",
    "for i, j in enumerate(np.random.randint(balanced['test']['X'].shape[0], size=(8,))):\n",
    "    \n",
    "    # Get an example image\n",
    "    X_sample = balanced['test']['X'][j,...]\n",
    "    \n",
    "    # Reshape flattened vector to image\n",
    "    X_reshape = np.reshape(X_sample, (66, 66))\n",
    "    \n",
    "    # Predict the label\n",
    "    y_pred = lc.predict(X_sample)\n",
    "    \n",
    "    # Display results\n",
    "    axes.flatten()[i].imshow(X_reshape, cmap='gray')\n",
    "    axes.flatten()[i].set_title('Label ' + str(balanced['test']['y'][j]) +', Prediction ' + str(np.argmax(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWgRBoPG3x1m",
    "outputId": "a8b98183-bbfb-4739-effa-f3c0d6f1e83e"
   },
   "outputs": [],
   "source": [
    "# Generate predictions and metrics for training data\n",
    "y_pred = lc.predict(balanced['train']['X'])\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_true = balanced['train']['y']\n",
    "\n",
    "metrics = benchmark_performance(y_true, y_pred)\n",
    "\n",
    "print('Training Recall: {}'.format(metrics['recall']))\n",
    "print('Training Precision: {}'.format(metrics['precision']))\n",
    "print('Training F1 Score: {}'.format(metrics['f1']),'\\n')\n",
    "\n",
    "# Visualize a confusion matrix\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(metrics['cm'])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "# Generate predictions and metrics for test data\n",
    "y_pred = lc.predict(balanced['test']['X'])\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_true = balanced['test']['y']\n",
    "\n",
    "metrics = benchmark_performance(y_true, y_pred)\n",
    "\n",
    "print('Testing Recall: {}'.format(metrics['recall']))\n",
    "print('Testing Precision: {}'.format(metrics['precision']))\n",
    "print('Testing F1 Score: {}'.format(metrics['f1']),'\\n')\n",
    "\n",
    "# Visualize a confusion matrix\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(metrics['cm'])\n",
    "disp.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXwiVLR4X_WY"
   },
   "source": [
    "# Part 2: Implementing a linear classifier with TensorFlow\n",
    "In this section, we will define our machine learning models using TensorFlow. These models are composed of layers - each layer specifies a mathematical operation that is applied to its input. The nice thing about TensorFlow is that almost all of the machinery required for stochastic gradient descent is taken care of for us.\n",
    "- Specify trainable variables? Check.\n",
    "- Initialize trainable variables with random values? Check.\n",
    "- Compute the layer outputs? Check.\n",
    "- Compute gradients using backpropagation? Check.\n",
    "- Perform all of the computations on GPUs to speed up training and inference? Check.\n",
    "All of the above (and more) are taken care of for us by TensorFlow - writing models often requires little math (although one practice that I encourage is keeping track of the input and output dimensions for each layer).\n",
    "\n",
    "To define a linear classifier, we will use a module in TensorFlow called Keras. Keras simple APIs for specifying models. In Keras, there are two different APIs you can use:\n",
    "- [Sequential API](https://www.tensorflow.org/guide/keras/sequential_model) - If your model is composed of a linear sequence of steps, this is the easier API to use.\n",
    "- [Functional API](https://www.tensorflow.org/guide/keras/functional) - If your model is more complicated, this API provides more flexibility. If you're using the functional API, consider using a class with methods to write submodels.\n",
    "The TensorFlow documentation provides additional details about how to use each of these two APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pkm3aspYX_WZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        \n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Activation, BatchNormalization, Conv2D, MaxPool2D, Softmax\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIHbR3PwX_WW"
   },
   "source": [
    "## Create dataset object\n",
    "TensorFlow uses Dataset objects to feed data into the training pipeline. These objects were covered in more detail in the TensorFlow Dataset notebook. In this section, we will make a class that builds a dataset object and applies random augmentation operation (e.g. rotation, flipping, scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "842TuxybX_WX"
   },
   "outputs": [],
   "source": [
    "# Create dataset builder\n",
    "class DatasetBuilder(object):\n",
    "    def __init__(self,\n",
    "                 X,\n",
    "                 y,\n",
    "                 batch_size=1,\n",
    "                 rotation_range=180,\n",
    "                 scale_range=(0.75, 1.25)):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.rotation_range = np.float(rotation_range)\n",
    "        self.scale_range = scale_range\n",
    "        \n",
    "        # Create dataset\n",
    "        self._create_dataset()\n",
    "        \n",
    "    def _augment(self, *args):\n",
    "        img = args[0]\n",
    "        label = args[1]\n",
    "        \n",
    "        theta = tf.random.uniform([1], 0, 2*np.pi*self.rotation_range/360)\n",
    "        img = tfa.image.rotate(img, theta)\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        \n",
    "        return (img, label)\n",
    "        \n",
    "    def _create_dataset(self):\n",
    "        X_train, X_temp, y_train, y_temp = sklearn.model_selection.train_test_split(self.X, self.y, train_size=0.8)\n",
    "        X_val, X_test, y_val, y_test = sklearn.model_selection.train_test_split(X_temp, y_temp, train_size=0.5)\n",
    "        \n",
    "        # Balance classes in each split\n",
    "        X_train, y_train = balance_classes(X_train, y_train)\n",
    "        X_test, y_test = balance_classes(X_test, y_test)\n",
    "        X_val, y_val = balance_classes(X_val, y_val)\n",
    "        \n",
    "        # Convert y data to categorical\n",
    "        y_train = tf.keras.utils.to_categorical(y_train)\n",
    "        y_test = tf.keras.utils.to_categorical(y_test)\n",
    "        y_val = tf.keras.utils.to_categorical(y_val)\n",
    "        \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "        \n",
    "        self.train_dataset = train_dataset.shuffle(256).batch(self.batch_size).map(self._augment)\n",
    "        self.val_dataset = val_dataset.batch(self.batch_size)\n",
    "        self.test_dataset = test_dataset.batch(self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Tq-Hg_mX_WX"
   },
   "outputs": [],
   "source": [
    "X, y = extract_classes(X_data, y_data, [3, 5])\n",
    "y[y == 3] = 0\n",
    "y[y == 5] = 1\n",
    "\n",
    "with tf.device('CPU:0'):\n",
    "    db = DatasetBuilder(X, y, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wbDlQmgBX_WY",
    "outputId": "c4e64f0d-686c-4210-ec11-b74cd391eebb"
   },
   "outputs": [],
   "source": [
    "# Check the data augmentation\n",
    "it = db.train_dataset.as_numpy_iterator()\n",
    "X_temp, y_temp = it.next()\n",
    "print(X_temp.shape)\n",
    "\n",
    "fig, axes = plt.subplots(4, figsize=(20,20))\n",
    "for i in range(4):\n",
    "    axes.flatten()[i].imshow(X_temp[i,...,:], cmap='Greys')\n",
    "    axes.flatten()[i].set_title('Label ' + str(y_temp[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-jix6tiX_WZ",
    "outputId": "a02646e0-7172-47c4-d57c-bdfb75e98ff9"
   },
   "outputs": [],
   "source": [
    "# Define the linear classifier\n",
    "def create_linear_classifier():\n",
    "    inputs = Input((X.shape[1], X.shape[2], 1),\n",
    "                   name='linear_classifier_input')\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dense(2)(x)\n",
    "    x = Softmax(axis=-1)(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "linear_classifier = create_linear_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaMQbitnX_Wa"
   },
   "source": [
    "## Specify training parameters \n",
    "In this section, we will specify how we want to train the neural network. We will need to specify three things:\n",
    "- The loss function. Because we are training a model for classification, we will use the categorical crossentropy\n",
    "- The training algorithm. There are many flavors of stochastic gradient descent - for this problem, we will use a variant called Adam\n",
    "- The training parameters. The training algorithm needs parameters like the learning rate, number of epochs, number of steps per epoch, etc. to be specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Li16VSCqX_Wb"
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ovxjMcTX_Wb"
   },
   "outputs": [],
   "source": [
    "# Define the training algorithm\n",
    "linear_optimizer = tf.keras.optimizers.Adam(lr=1e-3, clipnorm=0.001)\n",
    "fc_optimizer = tf.keras.optimizers.Adam(lr=1e-3, clipnorm=0.001)\n",
    "conv_optimizer = tf.keras.optimizers.Adam(lr=1e-3, clipnorm=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "veiaElEQX_Wb"
   },
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "training_steps_per_epoch=512\n",
    "n_epochs=32\n",
    "\n",
    "# Define callbacks\n",
    "linear_model_path = 'linear'\n",
    "\n",
    "linear_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        linear_model_path, monitor='val_loss',\n",
    "        save_best_only=True, verbose=1,\n",
    "        save_weights_only=False)\n",
    "]\n",
    "\n",
    "linear_callbacks.append(\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, verbose=1,\n",
    "        patience=3, min_lr=1e-7)\n",
    ")\n",
    "\n",
    "# Define metrics\n",
    "recall_0 = tf.keras.metrics.Recall(class_id=0)\n",
    "recall_1 = tf.keras.metrics.Recall(class_id=1)\n",
    "\n",
    "precision_0 = tf.keras.metrics.Precision(class_id=0)\n",
    "precision_1 = tf.keras.metrics.Precision(class_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile models\n",
    "linear_classifier.compile(optimizer=linear_optimizer, \n",
    "                          loss=loss_function, \n",
    "                          metrics = [recall_0, recall_1, precision_0, precision_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzRK8b8FX_Wb"
   },
   "source": [
    "## Train the model\n",
    "With the dataset, model, and training parameters defined, it is straightforward to train a model. Keras Model objects have a fit method that takes in the training parameters and executes the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rsv5dmnX_Wc",
    "outputId": "005cec17-54f5-4ef8-9e34-979732d43443"
   },
   "outputs": [],
   "source": [
    "# Train the linear classifier\n",
    "linear_classifier.fit(db.train_dataset,\n",
    "                      validation_data=db.val_dataset,\n",
    "                      epochs=n_epochs,\n",
    "                      verbose=1,\n",
    "                      callbacks=linear_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbiwMkypX_Wc"
   },
   "source": [
    "## Benchmark the model\n",
    "In this section, we will benchmark each model to assess the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BFhuK1YKX_Wd",
    "outputId": "f60e1843-02d5-4600-8889-db8cd7f3055e"
   },
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "it = db.test_dataset.as_numpy_iterator()\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(20,20))\n",
    "for i in range(8):\n",
    "    X_test, y_test = it.next()\n",
    "    \n",
    "    # Get an example image\n",
    "    X_sample = X_test[[i],...]\n",
    "    \n",
    "    # Predict the label\n",
    "    y_pred_linear = linear_classifier.predict(X_sample)\n",
    "    \n",
    "    # Display results\n",
    "    axes.flatten()[i].imshow(X_sample[0], cmap='gray')\n",
    "    axes.flatten()[i].set_title('Label ' + str(np.argmax(y_test[i])) +', Prediction ' + str(y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "test_list = list(db.test_dataset.as_numpy_iterator())\n",
    "X_test = np.concatenate([item[0] for item in test_list], axis=0)\n",
    "y_test = np.concatenate([item[1] for item in test_list], axis=0)\n",
    "y_test = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Compute linear classifier metrics\n",
    "y_pred = linear_classifier.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "metrics = benchmark_performance(y_test, y_pred)\n",
    "\n",
    "print('Testing Recall: {}'.format(metrics['recall']))\n",
    "print('Testing Precision: {}'.format(metrics['precision']))\n",
    "print('Testing F1 Score: {}'.format(metrics['f1']),'\\n')\n",
    "\n",
    "# Visualize a confusion matrix\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(metrics['cm'])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2.1\n",
    "\n",
    "Compare and contrast the performance of the first linear classifier that we trained to the version trained in tensorflow.\n",
    "\n",
    "########################  \n",
    "######## To Do ###########  \n",
    "########################  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "\n",
    "We will come back together as a group to discuss the exercise up to this point before moving on to the next notebook. If you have some extra time, try modifying the tensorflow model to work on 3 classes instead of just 2. At minimum, you will need to make the folloing modifications:\n",
    "- Change the `balance_classes` function to work on more than 2 classes\n",
    "- Update the initial input data to extract and assign 3 classes\n",
    "- Modify `create_linear_classifier` to work on 3 classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
